\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{letterpaper, left=5mm, top=5mm, right=5mm, bottom=5mm}
\usepackage{amsmath}
\usepackage{amssymb}
\DeclareMathOperator*{\argmax}{argmax}
\begin{document}
Reinforcement Learning Cheat Sheet
\iffalse \tiny \fi
\begin{itemize}
\item Part 1: Watch the Alpha Go Movie. Deep Mind. \textbf{Reinforcement Learning} - learning to make a good sequence of decisions. Atari, Robotics, Education, Healthcare, NLP, Computer Vision all use RL. 4 components of RL: \textbf{optimization} - about yielding best outcomes, \textbf{delayed consequences} - planning, sacrificing reward now for reward later, \textbf{exploration} - you don't know the consequences of decisions not taken so you need to learn from experience, \textbf{generalization} - programming policies to act in certain ways given certain situations. \textbf{AI planning} - involves optimization, delayed consequences, generalization (you know the rules). \textbf{supervised and unsupervised learning} - only optimization and generalization, just that unsupervised learning doesn't have labels. \textbf{imitation learning} - learns from experience of others, involves optimization, delayed consequences, and generalization, assumes an input of good policies, reduces RL to supervised learning. \textbf{Issues with RL} - having the right rewards, robustness vs risk sensitivity (exploration vs reward trade-off), multi-agent RL.  \textbf{sequential decision making} - cycle of: agent $\rightarrow$ (action $a_t$) $\rightarrow$ world $\rightarrow$ (observation $o_t$, reward $r_t$) $\rightarrow$ agent, goal is to select actions to maximize total future reward, may need to balance immediate and long term rewards, strategic behavior for high rewards, not the \textbf{discrete time step} is t. \textbf{history} - history of A, O, R that agent uses to make decisions. \textbf{world state} - full state of the world. \textbf{agent state} - state of world that agent needs to make decisions. \textbf{markov state} - state $s_t$ is Markov iff $P(s_{t+1} | s_t, a_t) = P(s_{t+1} | h_t, a_t)$ i.e. that the future is independent of the past given the present. Setting $s_t = h_t$ allows any world to be markov. In practice, most recent observation is sufficient statistic of history i.e. setting $s_t = o_t$. \textbf{full observability / MDP markov decision process} - agent state same as world state \textbf{partial observability / POMDP partially observable MDP} - agent constructs its own state i.e. $s_t = h_t$, beliefs, RNN, etc (poker players only sees own cards, healthcare doesn't see all physiological processes). \textbf{bandits seq decision process} - actions have no influence on next observations, no delayed rewards. \textbf{deterministic} - given history and action, single observation and reward. \textbf{stochastic} - given history and action, many possible (probability distribution of) observations and reward. \textbf{model} - agent's understanding (model) of how the world changes in response the agent taking an action. \textbf{policy} - map from agent state to action to take. \textbf{value function} - future rewards of being in a state and/or action when following a particular policy. \textbf{transition / dynamics model} - predicts agent next state $P(s_{t+1}=s' | s_t=s, a_t=a)$. \textbf{reward model} - predicts immediate reward $R(s_t=s, a_t=a) = E[r_t | s_t=s, a_t=a]$ (reward will depend on which state one probabilistically ends up at, that's why it's an expectation?). \textbf{deterministic policy} - $\pi(s)=a$. \textbf{stochastic policy} - $\pi(a|s) = P(a_t=a | s_t=s)$. \textbf{value function} - $V^\pi(s_t=s) = E_\pi[r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ... | s_t=s]$, \textbf{discount factor} $\gamma$ weights immediate vs. future rewards, expectation is taken over all the different paths that can be taken from the state s by the policy $\pi$. Types of RL agents (what the agent/algo learns) - \textbf{value based}: explicitly learns value function, implicitly learns policy; \textbf{policy based}: explicitly learns policy, there is no value function; \textbf{actor critic}: explicitly learns policy, explicitly learns value function. Types of RL agents - \textbf{model based}: has an explicit model, may or may not have a policy or value function; \textbf{model free}: explicit value function and/or policy function, no model. \textbf{planning} - algo computes how to act given model of world. \textbf{RL} - agent doesn't know how world works, interacts with world to explicitly/implicitly learn how it works, improves its policy. \textbf{exploration vs exploitation} - as the trade-off sounds. \textbf{evaluation} - estimate/predict expected rewards from following a given policy. \textbf{control} - optimization to find the best policy. 
\item Part 2: MDPs can model a huge number of interesting problems and settings. \textbf{bandits} - use single state MDPs. \textbf{optimal control} - mostly about continuous state MDPs. \textbf{POMDP} - state is history. \textbf{markov process / markov chain} - memoryless random process, sequence of random states with markov property, S - finite set of states, P - transition model, no rewards, no actions. \textbf{markov reward process} - S (finite), P, R - reward function, $\gamma$ - discount factor, no actions. \textbf{horizon} - number of time-steps in each episode in a process (can be a \textbf{finite} or infinite MRP). \textbf{return $G_t$ of MRP} - discounted sum of rewards from time-step t to horizon $G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + ... $. Note that $V(s) = E[G_t | s_t=s]$ (expectation over all paths that start at state s). \textbf{computing value of MRP empirically} - can simulate large number of episodes and average the returns. \textbf{bellman equation} - markov property of MRPs yields the following: $V(s) = R(s) + \gamma \sum \limits_{s' \in S} P(s' | s) V(s') $, i.e. the value at a state is the sum of \textit{the immediate reward} and \textit{the discounted sum of future rewards}. \textbf{matrix form bellman equation for finite state MRP} - $V = R + \gamma P V$. \textbf{analytic solution for value of MRP} - $V = (I - \gamma P)^{-1} R$, can be computed in $~O(n^3)$. \textbf{dynamic programming algo for computing value of an MRP} - (1) initialize $V_0(s) = 0$ for all s (2) for $k = 1$ until convergence, for all $s \in S$, $V_k(s) = R(s) + \gamma \sum \limits_{s' \in S} P(s' | s) V_{k-1}(s') $, algo is $O(s^2)$ for each $t$. In finite horizon case, $V_k^{\pi}$ is the exact value of k-horizon value of state s under policy $\pi$. In the infinite horizon case, $V_k^{\pi}(s)$ is an estimate of $ E_\pi[r_t + \gamma V_{i-1}| s_t=s]$ \textbf{markov decision process MDP} - is a tuple: (S [finite], A - actions [finite], P, R, $\gamma$). \textbf{MDP + $\pi(a|s)$ is a MRP} - (S, $R^\pi$, $P^\pi$, $\gamma$) where $R^\pi(s) = \sum \limits_{a \in A} \pi(a | s) R(s, a) $ and  $P^\pi(s' | s) = \sum \limits_{a \in A} \pi(a | s) P(s'|s, a) $. \textbf{modification of previous iterative algo for computing value of MDP + policy} - (1) initialize $V_0(s) = 0$ for all s (2) for $k = 1$ until convergence, for all $s \in S$, $V_k^\pi(s) = R^\pi(s) + \gamma \sum \limits_{s' \in S} P^\pi(s' | s) V^\pi_{k-1}(s') $ -- here a \textbf{bellman backup} for the policy $\pi$ is applied. \textbf{MDP Control} - compute optimal policy: $\pi^*(s) = \underset{\pi}{\operatorname{argmax}} V^\pi(s) $, the optimal policy for an MDP in an infinite horizon problem is deterministic, stationary (does not depend on time step), not necessarily unique. \textbf{policy search} - through enumeration searches $|A|^{|S|}$ deterministic policies. \textbf{state-action value of a policy} - take action a, then follow policy: $Q^\pi(s, a) = R(s, a) + \gamma \sum \limits_{s' \in S} P^\pi(s' | s, a) V^\pi(s')$. \textbf{policy iteration PI} - (1) i=0; init $\pi_0(s)$ randomly for all states $s$ (2) while i==0 or $|\pi_i - \pi_{i-1}| > 0$, do policy evaluation, policy improvement, i=i+1. \textbf{policy evaluation} - compute the value of $\pi_i$ using iterative algo (equivalent to applying Bellman repeatedly till convergence). \textbf{policy improvement} - $\pi_{i+1}(s) = \underset{a}{\operatorname{argmax}} Q^{\pi_i}(s, a) \forall s \in S$. We did this improvement step because we know $\underset{a}{\operatorname{max}} Q^{\pi_i}(s, a) >= V^{\pi_i}(s)$, but still that only suggests that following $\pi_{i+1}$ for one action and then following $\pi_i$ is better than just following $\pi_i$. However, it can be proved (by recursively plugging in the definition of Q) that $\pi_{i+1}$ provides \textbf{monotonic improvement} ($V^{\pi_{i+1}} >= V^{\pi_i}$ aka $V^{\pi_{i+1}}(s) >= V^{\pi_i}(s) \forall s \in S$) over $\pi_i$. PI can take at most $|A|^{|S|}$ iterations. \textbf{bellman backup} - applied to a value function, improves it if possible: $BV(s) = \underset{a}{\operatorname{max}} R(s, a) + \gamma \sum \limits_{s' \in S} P(s' | s, a) V(s') $ \textbf{value iteration VI} - considering longer and longer episodes to improve value function: (1) init $V_0(s) = 0 \forall s$ (2) set k = 1 (3) loop until finite horizon, convergence: (3a) for each state s, $V_{k+1}(s) = \underset{a}{\operatorname{max}} R(s, a) + \gamma \sum \limits_{s' \in S} P(s' | s, a) V_k(s')$ (essentially doing a bellman backup $V_{k+1} = BV_k$) (3b) $\pi_{k+1}(s) = \underset{a}{\operatorname{argmax}} R(s, a) + \gamma \sum \limits_{s' \in S} P(s' | s, a) V_k(s')$. \textbf{bellman for a particular policy} - $B^{\pi}V(s) = R^\pi(s) + \gamma \sum \limits_{s' \in S} P^\pi(s' | s) V(s') $, for policy evaluation repeatedly apply $B^\pi$, i.e. $V^\pi = B^\pi B^\pi ... B^\pi V$. \textbf{contraction operator} - $|OV - OV'| <= |V-V'|$. Bellman backup is a contraction on $V$, $|V-V'|$ is the \textbf{infinity norm}, the max difference over states $max(s) |V(s) - V'(s)|$. In VI for finite horizon k, optimal policy in general is not stationary (depends on time step). \textbf{VI vs PI} - VI: compute optimal policy for horizon=k and increment k, PI: compute infinite horizon value of policy (policy eval), use it to select a better policy (policy improvement).
\item Part 3: $G_t$, $V^{\pi}(s)$, and $Q^{\pi}(s,a)$ can be defined with respect to taking a particular policy $\pi$. \textbf{bootstrapping} - in the dynamic programming algo for policy eval, when you use a value estimate (in cache) for the future value $V_{i-1}$. \textbf{Monte Carlo policy evaluation} - generate a number of trajectories (state action paths till episode terminates) following policy $\pi$, average their returns to create a value estimate for the state -- doesn't need MDP dynamics/rewards?, no bootstrapping, state doesn't have to be Markov, only for episodic MDPs. \textbf{first visit MC on policy eval} - (1) after each episode i (1.1) define $G_{i,t}$ as the return from timestep $t$ onwards in the $i^{th}$ episode (1.2) for each state $s$ visited in episode $i$, for the first time $t$ that state $s$ is visited in episode $i$, increment counter of total first visits $N(s) = N(s) + 1$, increment total return $S(s) = S(s) + G_{i,t}$, update estimate $V^{\pi}(s) = S(s)/N(s)$ (2) By law of large numbers, as $N(s) \Rightarrow \inf, V^{\pi}(s) \Rightarrow E_\pi[G_t|s_t=s]$. \textbf{every visit MC on policy eval} - replace each instance of "first" in "first visit MC" description with "every". \textbf{incremental MC on policy eval} - same as "every visit MC" except update estimate should be done in the following way: $V^{\pi}(s) = V^{\pi}(s) * \frac{N(s)-1}{N(s)} + \frac{G_{i,t}}{N(s)} = V^{\pi}(s) + \frac{1}{N(s)}*(G_{i,t} - V^\pi(s))$. \textbf{incremental MC on policy eval running mean} - same as "incremental MC on policy eval" except update is: $V^{\pi}(s) = V^{\pi}(s) + \alpha*(G_{i,t} - V^\pi(s))$ where alpha can be manipulated -- if $\alpha > \frac{1}{N(s)}$, then you are forgetting older data. \textbf{MC off policy eval} - even though a behavior (old) policy will have different distribution of rewards across episodes, we use the behavior policy to estimate the value of the new policy -- useful when we don't have history for new policy like in medical field (not like \textit{we have an entirely new treatment for cancer} type of situation, but a situation like \textit{how would the outcome change if we had }). \textbf{bias} - $Bias_\theta(\hat{\theta}) = E_{x|\theta}[\hat{\theta}] - \theta$ \textbf{variance} - $Var(\hat{\theta}) = E_{x|\theta}[(\hat{\theta}-E[\hat{\theta}])^2]$ \textbf{MSE} - $MSE(\hat{\theta}) = Var(\hat{\theta}) + Bias_\theta(\hat{\theta})^2$ \textbf{Importance Sampling} - you have data $x_1 ... x_n$ sampled from distribution $p(x)$ and you have $E_{x \sim p}[f(x)]$ but you want $E_{x \sim q}[f(x)]$, we can show $E_{x \sim q}[f(x)] = \int_{x} q(x) f(x) dx$ which turns out to be $\approx \frac{1}{N} \sum_{i=1}^N \frac{q(x_i)}{p(x_i)} f(x_i)$ Note we could always evaluate q and p at particular data points, but we originally could not find the expectation of f with respect to the q distribution which is what we wanted. \textbf{Importance Sampling for Policy Evaluation (also off policy)} - $V^{\pi_1}(s) \approx \frac{1}{N} \sum_{j=1}^N \prod_t \frac{\pi_1(a_t|s_t)}{\pi_2(a_t|s_t)} G(h_j)$ so in MC policy eval we can use the empirical average or we can use a reweighted empirical average (importance sampling) as necessary. \textbf{temporal difference learning for estimating V} - $V^{\pi}(s_t) = V^{\pi}(s_t) + \alpha*([r_t + \gamma*V^{\pi}(s_{t+1})] - V^{\pi}(s_t))$ (note \textit{td error} is the expression in parens that $\alpha$ scales). \textbf{usability of methods} - no model: MC, TD; non episodic: DP, TD; non markov: MC, converges (when tabular, alpha < 1): DP, MC, TD; unbiased: MC; \textbf{tradeoffs} - TD lower variance because only 1 random decision, MC high variance no bias, TD only converges with tabular representation but MC converges even with functional representation. \textbf{MC/TD convergence} - MC converges to minimize MSE whereas TD converges to DP policy with maximum likelihood estimates for P(s'|s,a) and r(s,a). \textbf{certainty equivalence MLE MDP model estimates} - after each (s,a,r,s') tuple, recompute MLE MDP model for s,a; compute $V^\pi$ using MLE MDP model; data efficient but computationally expensive to update model. 
\item Part 4: \textbf{model free policy iteration} - we already know how to do policy eval model free; need to modify policy eval for deterministic policies because you can't compute $Q(s,a)$ when $\pi(s) \neq a$ $\Rightarrow$ so we need to try all $(s,a)$ pairs; have to interleave policy eval and policy improvement $\Rightarrow$ so we need to ensure that Q estimate is good enough so policy improvement is a monotonic operator. \textbf{$\epsilon$ greedy policy} -  with respect to state-action value $Q^\pi(s,a)$ is $\pi(a|s) = \begin{cases} a, & \text{with prob.}\ \frac{\epsilon}{|A|} \\ \underset{a}{\operatorname{argmax}} Q^\pi(s,a), & \text{with prob.} \frac{1 - \epsilon}{|A|} \end{cases}$. \textbf{Monotonic Improvement Thm for $\epsilon$-greedy policies} - For any $\epsilon$-greedy policy $\pi_{i+1}$ (the $\epsilon$-greedy policy w.r.t. $Q_{\pi_i}$), $\pi_{i+1}$ is a monotonic improvement: $V_{\pi_{i+1}} \geq V_{\pi_i}$.\textbf{Greedy in the Limit of Infinite Exploration (GLIE)} - all state-action pairs are visited an infinite number of times, a simple example would be $\epsilon$-greedy where $\epsilon$ is reduced to 0 at the following rate: $\epsilon_i = 1/i$. \textbf{Monte Carlo Online Control / On Policy Improvement} - Initialize $Q(s, a) = 0$, $Returns(s, a) = 0$ $\forall (s, a)$, Set $\epsilon = 1, k = 1$, $\pi_k = \epsilon$-greedy(Q) // Create initial $\epsilon$-greedy policy. loop. Sample k-th episode $(s_{k1}, a_{k1},r_{k1},s_{k2}, ... ,s_T)$ given $\pi_k$ for t = 1, ... , T do if First visit to (s, a) in episode k then Append $\sum_{j=t}^T r_{kj}$ to $Returns(s_t, a_t)$. $Q(s_t, a_t) = average(Returns(s_t, a_t))$ end if. end for. $k = k + 1$, $\epsilon = 1/k$. $\pi_k = \epsilon$-greedy($Q_\pi$) // Policy improvement. end loop. \textbf{GLIE MC converges} to the optimal state-action value function. \textbf{Our convention for this cheat sheet is to take an action from a state and then receive a reward.} Applying TD gives us \textbf{SARSA} - Set initial $\epsilon$-greedy policy $\pi$, t = 0, initial state $s_t = s_0$. Take $a_t \sim \pi(s_t)$ // Sample action from policy. Observe $(r_t, s_{t+1})$. loop. Take action $a_{t+1} \sim \pi(s_{t+1})$. Observe $(r_{t+1}, s_{t+2})$. Update Q given $(s_t, a_t ,r_t, s_{t+1}, a_{t+1})$: $Q^\pi(s_t, a_t) \Leftarrow Q^\pi(s_t, a_t) + \alpha*(r_t + \gamma*(Q^\pi(s_{t+1}, a_{t+1}) - Q^\pi(s_t, a_t)))$ Perform policy improvement: $\pi = \epsilon$-greedy($Q^\pi$). $t = t + 1$. end loop. \textbf{conditions for SARSA convergence} (1) policy sequence $\pi_t(a|s)$ satisfies the condition of GLIE (2) $\sum_t^{\infty} \alpha_t = \infty$ but $\sum_t^{\infty} \alpha_t^2 < \infty$. textbf{Importance Sampling for Off-Policy TD} - the new update is $V^{\pi_e}(s_t) = V^{\pi_e}(s_t) + \alpha*(\frac{\pi_e(a_t|s_t)}{\pi_b(a_t|s_t)}(r_t + \gamma*V^{\pi_e}(s_{t+1})-V^{\pi_e}(s_t)))$ -- we have lower variance than MC because there is just 1 ratio of $\pi$'s not a string of them we are multiplying, $\pi_b$ does not need to be the same at every time step, conditions: $\pi_b$ has the same support in that $\pi_b(a|s) > 0$ whenever $\pi_e(a|s)*V(s) > 0$. \textbf{Q Learning} - maintain state-action Q estimates and use to bootstrap-use the value of the best future action. Initialize $Q(s, a) \forall s \in S, a \in A$. $t = 0$, initial state $s_t = s_0$. Set $\pi_b$ to be $\epsilon$-greedy w.r.t. $Q$. loop. Take $a_t \sim \pi_b(s_t)$ // Sample action from policy. Observe $(r_t, s_{t+1})$. Update $Q$ given $(s_t, a_t, r_t, s_{t+1})$: $Q(s, a) \Leftarrow Q(s, a) + \alpha*(r_t + \gamma*\underset{a'}{max}(Q^(s_{t+1}, a') - Q^\pi(s, a)))$ Perform policy improvement: set $\pi_b$ to be $\epsilon$-greedy w.r.t. $Q$. $t = t + 1$. end loop. Q-Learning must visit all s,a pairs infinitely often to ensure convergence to the optimal q and must decay $\epsilon$ to ensure convergence to the optimal $\pi^*$. \textbf{Maximization Bias} - even though each estimate of the state-action values is unbiased in Q learning, the estimate of $\hat{\pi}$'s value $\hat{V}^{\hat{\pi}}$ can be biased because Jensen's inequality can be used to show that it is greater than or equal to $V^*$. \textbf{double Q learning} - split samples to create 2 unbiased estimates -- Initialize $Q_1(s, a)$ and $Q_2(s, a)$, $\forall s \in S$, $a \in A$, $t = 0$, initial state $s_t = s_0$. loop. Select $a_t$ using $\epsilon$-greedy $\pi(s) = \underset{a}{argmax} Q_1(s_t, a) + Q_2(s_t, a)$. Observe $(r_t, s_{t+1})$. if (with 0.5 probability) then $Q_1(s_t, a_t) \Leftarrow Q_1(s_t, a_t) + \alpha$ else $Q_2(s_t, a_t) \Leftarrow Q_2(s_t, a_t) + \alpha*(r_t + \gamma*Q_2*(s_{t+1}, \underset{a'}{max} Q_1(s_{t+1}, a')) - Q_2(s_t, a_t))$ end if. $t = t + 1$ end loop. 
\item Part 5: \textbf{value function approximation VFA} - represent a (state-action/state) value function with a parameterized function instead of a table -- reduce memory, computation, experience needed -- will use differentiable functions. \textbf{stochastic gradient descent SGD} - to find the parameter vector $w$ that minimizes the loss between a true value function $v_\pi(s)$ and its approximation $\hat{v}$ as represented with a particular function class parameterized by $w$. We use mean squared error and define \textbf{the loss} as $J(w) = E_\pi[(v_\pi(s) - \hat{v}(S,w))^2]$. \textbf{gradient descent} - we iteratively step in the direction of the negative gradient of the loss (loss and its gradient are calculated using all of the data we have). \textbf{stochastic gradient descent} - do gradient descent but compute the loss with a small fraction of the data points randomly picked (in our case we "sample" the gradient from particular randomly picked states, rather than taking the full expectation). Note $\triangledown_w J(w) = E_\pi[2(v_\pi(s) - \hat{v}(S,w))*\triangledown_w \hat{v}(S,w)]$ and $\triangledown_w J(w) = \alpha(v_\pi(s) - \hat{v}(S,w))*\triangledown_w \hat{v}(S,w)$ for a particular state S. \textbf{passive reinforcement learning} - have to use an estimate for $v_\pi(s)$ since we don't have it, this is policy evaluation without a model, "passive" because not trying to learn the optimal decision policy.  
\end{itemize}
\end{document}
